{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae28787",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362e83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "# from src.datasets import ARDataset\n",
    "import datasets\n",
    "from src.utils import create_noisy_ar_tokenizer\n",
    "\n",
    "from src.utils import ObjectView, get_cls_by_name, get_optimizer, get_fn_param_names\n",
    "\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import contextlib\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52cf492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ObjectView(dict(\n",
    "    seed = 42,\n",
    "    save_path = \"/home/user36/metamem/runs\",\n",
    "\n",
    "    # LM model args\n",
    "    arch = \"gpt_neox\",\n",
    "    hidden_size = 128,\n",
    "    num_hidden_layers = 4,\n",
    "    num_attention_heads = 4,\n",
    "\n",
    "    # AR dataset args\n",
    "    num_symbols = 16,\n",
    "    key_size = 2,\n",
    "    value_size = 1,\n",
    "    num_pairs = 16,\n",
    "    ar_mode = \"remember\",\n",
    "\n",
    "    pretrain_size = 100000,\n",
    "    train_size = 100000,\n",
    "    valid_size = 1000,\n",
    "    test_size = 10000,\n",
    "    data_n_workers = 4,\n",
    "\n",
    "    # meta memory args\n",
    "    num_mem_tokens = 4,\n",
    "    use_lora = False,\n",
    "    max_inner_iter = 1000,\n",
    "    inner_target_loss = 0.0,\n",
    "\n",
    "    # train args\n",
    "    iters = 10000,\n",
    "    log_interval = 100,\n",
    "    valid_interval = 500,\n",
    "    batch_size = 128,\n",
    "    gradient_accumulation_steps = 1,\n",
    "\n",
    "    # optimizer args\n",
    "    inner_optimizer = \"SGD\",\n",
    "    inner_lr = 1e-3,\n",
    "    inner_momentum = 0.9,\n",
    "    inner_weight_decay = 1e-2,\n",
    "    nesterov = True,\n",
    "\n",
    "    optimizer = \"AdamW\",\n",
    "    lr = 3e-4,\n",
    "    weight_decay = 1e-2,\n",
    "    lr_scheduler = \"linear\",\n",
    "    # num_warmup_steps = 1000,\n",
    "\n",
    "    best_metric_value = 1.0,\n",
    "    optimize_mode = 'max',\n",
    "))\n",
    "\n",
    "args['num_warmup_steps'] = args['iters'] // 10\n",
    "args_cp = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1714ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer):\n",
    "\n",
    "    query = [item['context'] + item['query'] + item['target'] for item in batch]\n",
    "    query_input_ids = tokenizer(query, return_tensors=\"pt\", add_special_tokens=False,\n",
    "                                padding=True, pad_to_multiple_of=8).input_ids\n",
    "    # target_ids = tokenizer([item['target'] for item in batch], return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    # add labels_mask\n",
    "    # input_seq: 0, target_seq: 1, seq = input_seq + target_seq\n",
    "    labels_mask = torch.zeros_like(query_input_ids)\n",
    "    for i, item in enumerate(batch):\n",
    "        # context_seq_len = len(item['context'])\n",
    "        query_seq_len = len(item['query']) + len(item['context'])\n",
    "        target_seq_len = len(item['target'])\n",
    "        labels_mask[i, query_seq_len:query_seq_len + target_seq_len] = 1\n",
    "\n",
    "    labels = query_input_ids * labels_mask + (1 - labels_mask) * -100\n",
    "    return {\n",
    "        'input_ids': query_input_ids,\n",
    "        'labels': labels,\n",
    "        'labels_mask': labels_mask.bool(),\n",
    "        # 'target_ids': target_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f0bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37020594992048c2be07e0b9dbcf253c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d2676a9ff5460cb00690563df2ae47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/45.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5442a8fac04ac0844d287e87345fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/231k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d4730c5bca45729de531817ae00545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e77728a3ba4d8092962a5eedae7233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kwargs = {'pin_memory': True, 'num_workers': args.data_n_workers}\n",
    "\n",
    "ar_dataset = datasets.load_dataset(\"yurakuratov/N2-K4V4-S1_16-32_1M\")\n",
    "pretrain_dataset = ar_dataset[\"train\"].select(range(100_000))\n",
    "valid_dataset = ar_dataset[\"valid\"]\n",
    "\n",
    "train_rnd_generator = torch.Generator()\n",
    "train_rnd_generator.manual_seed(args.seed)\n",
    "per_worker_batch_size = args.batch_size * args.gradient_accumulation_steps\n",
    "kwargs = {'pin_memory': True, 'num_workers': args.data_n_workers}\n",
    "\n",
    "\n",
    "tokenizer = create_noisy_ar_tokenizer()\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "bos_token_id = tokenizer.convert_tokens_to_ids('[BOS]')\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids('[EOS]')\n",
    "\n",
    "collate_fn_caller = lambda batch: collate_fn(batch, tokenizer)\n",
    "\n",
    "pretrain_dataloader = DataLoader(pretrain_dataset, batch_size=per_worker_batch_size, generator=train_rnd_generator,\n",
    "                                 collate_fn=collate_fn_caller, **kwargs)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=per_worker_batch_size,\n",
    "                        collate_fn=collate_fn_caller, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5057ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %run ../src/configs/base_models/create_config.py --arch gpt_neox --hidden_size 128 --num_hidden_layers 4 --num_attention_heads 4\n",
    "\n",
    "cfg_path = f\"/home/user36/metamem/src/configs/base_models/exp/{args.arch}_tiny_{args.num_hidden_layers}l{args.num_attention_heads}hd{args.hidden_size}.json\"\n",
    "model_cfg = AutoConfig.from_pretrained(cfg_path)\n",
    "\n",
    "model_cls = get_cls_by_name(f\"transformers:{model_cfg.architectures[0]}\")\n",
    "model = model_cls(config=model_cfg)\n",
    "# sd = torch.load(\"/home/user36/metamem/runs/neox_ar_simple.pth\")\n",
    "# model.load_state_dict(sd)\n",
    "args = args_cp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer_cls = get_optimizer(args.optimizer)\n",
    "optimizer = optimizer_cls(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "lr_scheduler = get_scheduler(args.lr_scheduler, optimizer, args.num_warmup_steps, args.iters * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d1da057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in pretrain_dataloader:\n",
    "#     break\n",
    "\n",
    "# for k, v in batch.items():\n",
    "#     batch[k] = v.to(device)\n",
    "\n",
    "# outputs = model(\n",
    "#     input_ids=batch['input_ids'],\n",
    "#     labels=batch['labels'],\n",
    "#     labels_mask=batch['labels_mask'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9cd8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_token_ids = [tokenizer.convert_tokens_to_ids(t) for t in ['!', '|']] + [-100]\n",
    "\n",
    "def compute_accuracy(logits, input_ids, labels_mask):\n",
    "    preds = torch.argmax(logits.detach(), dim=-1)\n",
    "    for t_id in ignore_token_ids:\n",
    "        labels_mask &= (input_ids != t_id)\n",
    "\n",
    "    \n",
    "    shift_labels = input_ids[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous().argmax(dim=-1)\n",
    "\n",
    "    shift_mask = labels_mask[..., 1:].contiguous()\n",
    "\n",
    "    # target_values = [p[m] for p, m in zip(input_ids.cpu(), labels_mask.cpu())]\n",
    "    # pred_labels = [p[m] for p, m in zip(preds.cpu(), labels_mask.cpu())]\n",
    "\n",
    "\n",
    "    target_values = [p[m] for p, m in zip(shift_labels.cpu(), shift_mask.cpu())]\n",
    "    pred_labels = [p[m] for p, m in zip(shift_logits.cpu(), shift_mask.cpu())]\n",
    "\n",
    "    # print(target_values[:16])\n",
    "    # print(pred_labels[:16])\n",
    "\n",
    "    correct = np.sum([torch.all(text == pred).cpu().item() for text, pred in zip(target_values, pred_labels)])\n",
    "    total = len(target_values)\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    return acc, correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6e956a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # move to device\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                labels=batch['labels'],\n",
    "                labels_mask=batch['labels_mask'],\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            labels_mask = batch['labels_mask']\n",
    "            shift_labels = batch['labels'][..., 1:].contiguous()\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "\n",
    "            shift_mask = labels_mask[..., 1:].contiguous()\n",
    "            flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "            flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "        \n",
    "            loss = F.cross_entropy(flat_logits, flat_labels, ignore_index=-100)\n",
    "\n",
    "            acc, correct, total = compute_accuracy(logits, batch['input_ids'], labels_mask)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_correct += correct\n",
    "            val_total += total\n",
    "\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "    avg_acc = 100.0 * val_correct / val_total\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5e1c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "def inf_loop(dataloader):\n",
    "    for loader in repeat(dataloader):\n",
    "        yield from loader\n",
    "\n",
    "def fit(model, train_dataloader, valid_dataloader, optimizer, scheduler, device, args):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    pbar = tqdm.auto.tqdm(total=args.iters, desc='Train')\n",
    "    for step, batch in enumerate(inf_loop(train_dataloader)):\n",
    "    # for step, batch in \n",
    "        pbar.update(1)\n",
    "\n",
    "        # move to device\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        # forward + backward + step\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            labels=batch['labels'],\n",
    "            labels_mask=batch['labels_mask'],\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        labels_mask = batch['labels_mask']\n",
    "        shift_labels = batch['labels'][..., 1:].contiguous()\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        flat_labels = shift_labels.view(-1)\n",
    "        flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "\n",
    "        shift_mask = labels_mask[..., 1:].contiguous()\n",
    "        flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "        flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "    \n",
    "        loss = F.cross_entropy(flat_logits, flat_labels, ignore_index=-100)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # accumulate for logging\n",
    "        acc, correct, total = compute_accuracy(logits, batch['input_ids'], labels_mask)\n",
    "        running_loss += loss.item()\n",
    "        running_correct += correct\n",
    "        running_total += total\n",
    "\n",
    "        # train-side logging\n",
    "        if step % args.log_interval == 0 or step == args.iters:\n",
    "            elapsed = (time.time() - start_time) / step if step > 0 else 0\n",
    "            avg_loss = running_loss / args.log_interval\n",
    "            avg_acc = 100.0 * running_correct / running_total if running_total > 0 else 0.0\n",
    "            print(f\"[Train] Step {step:5d}/{args.iters:5d} • \"\n",
    "                  f\"Loss: {avg_loss:.4f} • Acc: {avg_acc:5.2f}% • \"\n",
    "                  f\"{elapsed:.3f}s/step\")\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            running_total = 0\n",
    "\n",
    "        # periodic validation\n",
    "        if step % args.valid_interval == 0:\n",
    "            val_loss, val_acc = validate(model, valid_dataloader, device)\n",
    "            print(f\"⏸ [Valid] after {step} steps → \"\n",
    "                  f\"Val Loss: {val_loss:.4f} • Val Acc: {val_acc:5.2f}%\")\n",
    "            # save best\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f\"{args.save_path}/model.pth\")\n",
    "                print(f\"  ✔ New best model saved at step {step} → {args.save_path}\")\n",
    "        # break\n",
    "        if step == args.iters:\n",
    "            break\n",
    "\n",
    "    # final validation at end if not aligned to interval\n",
    "    if args.iters % args.valid_interval != 0:\n",
    "        val_loss, val_acc = validate(model, valid_dataloader, device)\n",
    "        print(f\"⏸ [Valid] final → Val Loss: {val_loss:.4f} • Val Acc: {val_acc:5.2f}%\")\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save(model.state_dict(), f\"{args.save_path}/model.pth\")\n",
    "            print(f\"  ✔ New best model saved at final step → {args.save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d8f37535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': typing.Optional[torch.Tensor], 'generation_config': typing.Optional[transformers.generation.configuration_utils.GenerationConfig], 'logits_processor': typing.Optional[transformers.generation.logits_process.LogitsProcessorList], 'stopping_criteria': typing.Optional[transformers.generation.stopping_criteria.StoppingCriteriaList], 'prefix_allowed_tokens_fn': typing.Optional[typing.Callable[[int, torch.Tensor], list[int]]], 'synced_gpus': typing.Optional[bool], 'assistant_model': typing.Optional[ForwardRef('PreTrainedModel')], 'streamer': typing.Optional[ForwardRef('BaseStreamer')], 'negative_prompt_ids': typing.Optional[torch.Tensor], 'negative_prompt_attention_mask': typing.Optional[torch.Tensor], 'use_model_defaults': typing.Optional[bool], 'custom_generate': typing.Optional[str], 'return': typing.Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor]}\n"
     ]
    }
   ],
   "source": [
    "print(model.generate.__annotations__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7a55d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[16, 51, 17,  8, 64, 47, 66, 49, 36,  6, 50, 68, 10, 11, 40, 65, 66, 33,\n",
       "         17, 52, 69, 67, 66, 49, 36,  6, 50, 68]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d406fa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ghK9!|'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_dataset[0]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b61d63cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mVne8R!TGcU:ghK9!DnW|?!TGcU:ghK9!|'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_dataset[0][\"context\"] + pretrain_dataset[0][\"query\"] + pretrain_dataset[0]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0099240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mVne8R!TGcU:ghK9!DnW|?!TGcU:ghK9!|hhh!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = pretrain_dataset[0][\"context\"] + pretrain_dataset[0][\"query\"]\n",
    "input_ids = tokenizer(query, return_tensors=\"pt\")\n",
    "generate_kwargs = {\"pad_token_id\": -100, \"max_new_tokens\":10}\n",
    "tokenizer.decode(model.generate(inputs=input_ids['input_ids'].cuda(), max_new_tokens=10).tolist()[0]).replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab70cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!!!!!!|!!!!m!!!!|!!!!!|K!!!ghK9!|h'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = pretrain_dataset[0][\"context\"] + pretrain_dataset[0][\"query\"] + pretrain_dataset[0]['target']\n",
    "\n",
    "input_ids = tokenizer(query, return_tensors=\"pt\")\n",
    "generate_kwargs = {\"pad_token_id\": -100, \"max_new_tokens\":10}\n",
    "tokenizer.decode(model(input_ids['input_ids'].cuda()).logits.argmax(dim=-1).tolist()[0]).replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddac21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a7313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "for batch in valid_dataloader:\n",
    "    break\n",
    "\n",
    "for k, v in batch.items():\n",
    "    batch[k] = v.to(device)\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=batch['input_ids'],\n",
    "    labels=batch['labels'],\n",
    "    labels_mask=batch['labels_mask'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea9d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2c4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metamem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
